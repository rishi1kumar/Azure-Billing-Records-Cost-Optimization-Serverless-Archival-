
# Azure Billing Records Cost Optimization (Serverless + Archival)

## ðŸ“˜ Problem Statement
We operate a **read-heavy serverless billing system** built on Azure, storing over 2 million billing records in **Azure Cosmos DB**. Each record is up to **300KB** in size. However, **records older than 3 months are rarely accessed**, and the system has become increasingly expensive.

## Objective
Optimize costs while ensuring:
- âœ… Zero data loss
- âœ… No API contract changes
- âœ… Seamless transition & no downtime
- âœ… Response time for old records: seconds
- âœ… Easy maintainability

---

##  Solution Overview
We implement **hot/cold storage tiering**:
- Hot: Azure Cosmos DB for recent (â‰¤3 months) records
- Cold: Azure Blob Storage (Cool/Archive tier) for historical records

### Architecture
![Architecture Diagram](diagrams/architecture.png)

---

## Repository Structure
```bash
azure-billing-archival/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ archive_old_records/        # Timer-triggered archival logic
â”‚   â””â”€â”€ retrieve_record/            # Read API with Cosmos + Blob fallback
â”œâ”€â”€ infra/                          # Bicep or Terraform for Infra deployment
â”œâ”€â”€ utils/                          # Reusable blob helpers
â”œâ”€â”€ diagrams/                       # Architecture images
â””â”€â”€ scripts/                        # CLI to manage blob storage
```

---

## Setup Instructions

### 1. Clone & Install
```bash
git clone https://github.com/yourusername/azure-billing-archival.git
cd azure-billing-archival
```

### 2. Deploy Infrastructure (Example: Bicep)
```bash
az deployment group create \
  --resource-group my-rg \
  --template-file infra/main.bicep \
  --parameters @infra/parameters.json
```

### 3. Deploy Azure Functions
```bash
cd functions/archive_old_records
func azure functionapp publish <your-function-app>

cd ../retrieve_record
func azure functionapp publish <your-function-app>
```

---

## Archival Flow
- A timer-triggered function runs monthly
- Moves records older than 3 months from Cosmos DB to Blob Storage (as JSON)
- Inserts a metadata stub with blob reference into Cosmos DB

### Sample Pseudocode
```python
if record.timestamp < cutoff_date:
    blob_url = upload_to_blob(record)
    save_stub_to_cosmos(record.id, blob_url)
```

---

##  Retrieval Flow
- Read API first queries Cosmos DB
- If a stub with `archived: true` is found, download the full record from Blob Storage

---

## ðŸ’° Cost Optimization Strategies
- Cosmos DB RUs are minimized (older data is offloaded)
- Blob Storage tiers:
  - **Hot**: short-term
  - **Cool**: long-term infrequent access
  - **Archive**: cold access, lowest cost

### Tier Management (Sample Script)
```bash
./scripts/set_blob_tier.sh archive <blob-name>
```

---

##  API Compatibility
No changes to input/output or interface. API users interact as if all data resides in Cosmos DB.

---

##  Technologies Used
- Azure Cosmos DB
- Azure Blob Storage (Cool/Archive Tier)
- Azure Functions (Timer + HTTP Trigger)
- Bicep / Terraform (for infra deployment)
- Python SDKs (Cosmos + Blob)

---

##  License
MIT

---

##  Contributions & Contact
PRs welcome. For feedback or questions, contact: `your.email@example.com`
